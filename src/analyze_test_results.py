#!/usr/bin/env python3
"""
í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ìœ„í˜‘ ì‹œë‚˜ë¦¬ì˜¤ í…ŒìŠ¤íŠ¸ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ìƒì„¸í•œ ë³´ê³ ì„œë¥¼ ìƒì„±
"""

import json
import logging
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import matplotlib.pyplot as plt
import seaborn as sns

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TestResultAnalyzer:
    """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ê¸°"""
    
    def __init__(self, results_dir: str = "threat_test_results"):
        self.results_dir = Path(results_dir)
        self.results_dir.mkdir(exist_ok=True)
        
        # ê²°ê³¼ ë°ì´í„°
        self.scenario_results = {}
        self.summary_data = None
        self.analysis_results = {}
        
    def load_results(self) -> bool:
        """ê²°ê³¼ íŒŒì¼ ë¡œë“œ"""
        try:
            # ìš”ì•½ íŒŒì¼ ì°¾ê¸°
            summary_files = list(self.results_dir.glob("test_summary_*.json"))
            if not summary_files:
                logger.error("ìš”ì•½ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # ê°€ì¥ ìµœê·¼ ìš”ì•½ íŒŒì¼ ë¡œë“œ
            latest_summary = max(summary_files, key=lambda x: x.stat().st_mtime)
            with open(latest_summary, 'r', encoding='utf-8') as f:
                self.summary_data = json.load(f)
            
            logger.info(f"ìš”ì•½ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {latest_summary}")
            
            # ì‹œë‚˜ë¦¬ì˜¤ë³„ ê²°ê³¼ íŒŒì¼ ë¡œë“œ
            scenario_files = list(self.results_dir.glob("*_*.json"))
            scenario_files = [f for f in scenario_files if not f.name.startswith("test_summary_")]
            
            for scenario_file in scenario_files:
                scenario_name = scenario_file.name.split('_')[0]
                with open(scenario_file, 'r', encoding='utf-8') as f:
                    self.scenario_results[scenario_name] = json.load(f)
            
            logger.info(f"ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ íŒŒì¼ {len(self.scenario_results)}ê°œ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ê²°ê³¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def analyze_performance_metrics(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ ì§€í‘œ ë¶„ì„"""
        if not self.summary_data:
            return {}
        
        analysis = {
            'overall_performance': self.summary_data.get('overall_performance', {}),
            'scenario_analysis': {},
            'platform_analysis': {},
            'threat_type_analysis': {}
        }
        
        # ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¶„ì„
        for scenario_name, scenario_summary in self.summary_data.get('scenario_summaries', {}).items():
            if 'error' in scenario_summary:
                continue
            
            analysis['scenario_analysis'][scenario_name] = {
                'avg_precision': scenario_summary.get('avg_precision', 0),
                'avg_recall': scenario_summary.get('avg_recall', 0),
                'avg_f1_score': scenario_summary.get('avg_f1_score', 0),
                'avg_accuracy': scenario_summary.get('avg_accuracy', 0),
                'platforms_tested': scenario_summary.get('platforms_tested', 0)
            }
        
        # í”Œë«í¼ë³„ ë¶„ì„
        platforms = self.summary_data.get('platforms', [])
        for platform in platforms:
            platform_metrics = []
            
            for scenario_name, scenario_summary in self.summary_data.get('scenario_summaries', {}).items():
                if 'error' in scenario_summary:
                    continue
                
                platform_result = scenario_summary.get('platform_results', {}).get(platform, {})
                if 'error' not in platform_result:
                    platform_metrics.append({
                        'scenario': scenario_name,
                        'precision': platform_result.get('precision', 0),
                        'recall': platform_result.get('recall', 0),
                        'f1_score': platform_result.get('f1_score', 0),
                        'accuracy': platform_result.get('accuracy', 0)
                    })
            
            if platform_metrics:
                df = pd.DataFrame(platform_metrics)
                analysis['platform_analysis'][platform] = {
                    'avg_precision': df['precision'].mean(),
                    'avg_recall': df['recall'].mean(),
                    'avg_f1_score': df['f1_score'].mean(),
                    'avg_accuracy': df['accuracy'].mean(),
                    'std_precision': df['precision'].std(),
                    'std_recall': df['recall'].std(),
                    'std_f1_score': df['f1_score'].std(),
                    'std_accuracy': df['accuracy'].std()
                }
        
        return analysis
    
    def analyze_threat_detection_patterns(self) -> Dict[str, Any]:
        """ìœ„í˜‘ íƒì§€ íŒ¨í„´ ë¶„ì„"""
        analysis = {
            'detection_rates': {},
            'false_positive_rates': {},
            'false_negative_rates': {},
            'threat_type_effectiveness': {}
        }
        
        for scenario_name, scenario_data in self.scenario_results.items():
            if 'error' in scenario_data:
                continue
            
            for platform_name, platform_data in scenario_data.items():
                if 'error' in platform_data:
                    continue
                
                # íƒì§€ìœ¨ ë¶„ì„
                detection_rate = platform_data.get('detection_rate', 0)
                if scenario_name not in analysis['detection_rates']:
                    analysis['detection_rates'][scenario_name] = {}
                analysis['detection_rates'][scenario_name][platform_name] = detection_rate
                
                # False Positive Rate
                fp_rate = platform_data.get('false_positives', 0) / max(platform_data.get('total_events', 1), 1)
                if scenario_name not in analysis['false_positive_rates']:
                    analysis['false_positive_rates'][scenario_name] = {}
                analysis['false_positive_rates'][scenario_name][platform_name] = fp_rate
                
                # False Negative Rate
                fn_rate = platform_data.get('false_negatives', 0) / max(platform_data.get('actual_threats', 1), 1)
                if scenario_name not in analysis['false_negative_rates']:
                    analysis['false_negative_rates'][scenario_name] = {}
                analysis['false_negative_rates'][scenario_name][platform_name] = fn_rate
                
                # ìœ„í˜‘ ìœ í˜•ë³„ íš¨ê³¼ì„±
                threat_types = platform_data.get('threat_types', {})
                for threat_type, count in threat_types.items():
                    if threat_type not in analysis['threat_type_effectiveness']:
                        analysis['threat_type_effectiveness'][threat_type] = {}
                    if scenario_name not in analysis['threat_type_effectiveness'][threat_type]:
                        analysis['threat_type_effectiveness'][threat_type][scenario_name] = {}
                    analysis['threat_type_effectiveness'][threat_type][scenario_name][platform_name] = count
        
        return analysis
    
    def generate_visualizations(self) -> List[str]:
        """ì‹œê°í™” ìƒì„±"""
        generated_files = []
        
        try:
            # 1. ì „ì²´ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸
            self._create_performance_comparison_chart()
            generated_files.append("performance_comparison.png")
            
            # 2. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ì°¨íŠ¸
            self._create_scenario_performance_chart()
            generated_files.append("scenario_performance.png")
            
            # 3. í”Œë«í¼ë³„ ì„±ëŠ¥ ì°¨íŠ¸
            self._create_platform_performance_chart()
            generated_files.append("platform_performance.png")
            
            # 4. ìœ„í˜‘ íƒì§€ íŒ¨í„´ íˆíŠ¸ë§µ
            self._create_detection_pattern_heatmap()
            generated_files.append("detection_pattern_heatmap.png")
            
            # 5. ROC ê³¡ì„  (ê°€ëŠ¥í•œ ê²½ìš°)
            self._create_roc_curves()
            generated_files.append("roc_curves.png")
            
        except Exception as e:
            logger.error(f"ì‹œê°í™” ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
        
        return generated_files
    
    def _create_performance_comparison_chart(self):
        """ì „ì²´ ì„±ëŠ¥ ë¹„êµ ì°¨íŠ¸ ìƒì„±"""
        if not self.summary_data:
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('DID ìœ„í˜‘ íƒì§€ ì‹œìŠ¤í…œ ì „ì²´ ì„±ëŠ¥ ë¹„êµ', fontsize=16, fontweight='bold')
        
        metrics = ['precision', 'recall', 'f1_score', 'accuracy']
        metric_names = ['Precision', 'Recall', 'F1-Score', 'Accuracy']
        
        for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            ax = axes[i//2, i%2]
            
            scenario_data = []
            platform_data = []
            values = []
            
            for scenario_name, scenario_summary in self.summary_data.get('scenario_summaries', {}).items():
                if 'error' in scenario_summary:
                    continue
                
                for platform_name, platform_result in scenario_summary.get('platform_results', {}).items():
                    if 'error' not in platform_result:
                        scenario_data.append(scenario_name)
                        platform_data.append(platform_name)
                        values.append(platform_result.get(metric, 0))
            
            if values:
                df = pd.DataFrame({
                    'Scenario': scenario_data,
                    'Platform': platform_data,
                    'Value': values
                })
                
                # í”Œë«í¼ë³„ í‰ê· ê°’ìœ¼ë¡œ ê·¸ë£¹í™”
                platform_avg = df.groupby('Platform')['Value'].mean().sort_values(ascending=False)
                
                bars = ax.bar(platform_avg.index, platform_avg.values, 
                             color=['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4'][:len(platform_avg)])
                
                ax.set_title(f'{metric_name} ë¹„êµ')
                ax.set_ylabel(metric_name)
                ax.set_ylim(0, 1)
                
                # ê°’ í‘œì‹œ
                for bar, value in zip(bars, platform_avg.values):
                    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                           f'{value:.3f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'performance_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_scenario_performance_chart(self):
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±"""
        if not self.summary_data:
            return
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        scenarios = []
        f1_scores = []
        colors = []
        
        for scenario_name, scenario_summary in self.summary_data.get('scenario_summaries', {}).items():
            if 'error' in scenario_summary:
                continue
            
            scenarios.append(scenario_name.replace('_', ' ').title())
            f1_scores.append(scenario_summary.get('avg_f1_score', 0))
            
            # ì„±ëŠ¥ì— ë”°ë¥¸ ìƒ‰ìƒ ì„¤ì •
            f1_score = scenario_summary.get('avg_f1_score', 0)
            if f1_score >= 0.8:
                colors.append('#96CEB4')  # ë…¹ìƒ‰
            elif f1_score >= 0.6:
                colors.append('#FFEAA7')  # ë…¸ë€ìƒ‰
            else:
                colors.append('#FF6B6B')  # ë¹¨ê°„ìƒ‰
        
        bars = ax.bar(scenarios, f1_scores, color=colors)
        ax.set_title('ì‹œë‚˜ë¦¬ì˜¤ë³„ í‰ê·  F1-Score', fontsize=14, fontweight='bold')
        ax.set_ylabel('F1-Score')
        ax.set_ylim(0, 1)
        
        # ê°’ í‘œì‹œ
        for bar, value in zip(bars, f1_scores):
            ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01, 
                   f'{value:.3f}', ha='center', va='bottom')
        
        # xì¶• ë ˆì´ë¸” íšŒì „
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        plt.savefig(self.results_dir / 'scenario_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_platform_performance_chart(self):
        """í”Œë«í¼ë³„ ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±"""
        if not self.summary_data:
            return
        
        fig, ax = plt.subplots(figsize=(10, 6))
        
        platforms = self.summary_data.get('platforms', [])
        metrics = ['precision', 'recall', 'f1_score', 'accuracy']
        metric_names = ['Precision', 'Recall', 'F1-Score', 'Accuracy']
        
        x = np.arange(len(platforms))
        width = 0.2
        
        for i, (metric, metric_name) in enumerate(zip(metrics, metric_names)):
            values = []
            
            for platform in platforms:
                platform_metrics = []
                for scenario_name, scenario_summary in self.summary_data.get('scenario_summaries', {}).items():
                    if 'error' in scenario_summary:
                        continue
                    
                    platform_result = scenario_summary.get('platform_results', {}).get(platform, {})
                    if 'error' not in platform_result:
                        platform_metrics.append(platform_result.get(metric, 0))
                
                avg_value = np.mean(platform_metrics) if platform_metrics else 0
                values.append(avg_value)
            
            ax.bar(x + i*width, values, width, label=metric_name)
        
        ax.set_xlabel('Platform')
        ax.set_ylabel('Score')
        ax.set_title('í”Œë«í¼ë³„ í‰ê·  ì„±ëŠ¥ ì§€í‘œ')
        ax.set_xticks(x + width * 1.5)
        ax.set_xticklabels(platforms)
        ax.legend()
        ax.set_ylim(0, 1)
        
        plt.tight_layout()
        plt.savefig(self.results_dir / 'platform_performance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_detection_pattern_heatmap(self):
        """íƒì§€ íŒ¨í„´ íˆíŠ¸ë§µ ìƒì„±"""
        if not self.scenario_results:
            return
        
        # ë°ì´í„° ì¤€ë¹„
        scenarios = []
        platforms = []
        detection_rates = []
        
        for scenario_name, scenario_data in self.scenario_results.items():
            if 'error' in scenario_data:
                continue
            
            for platform_name, platform_data in scenario_data.items():
                if 'error' in platform_data:
                    continue
                
                scenarios.append(scenario_name)
                platforms.append(platform_name)
                detection_rates.append(platform_data.get('detection_rate', 0))
        
        if not detection_rates:
            return
        
        # ë°ì´í„°í”„ë ˆì„ ìƒì„±
        df = pd.DataFrame({
            'Scenario': scenarios,
            'Platform': platforms,
            'Detection Rate': detection_rates
        })
        
        # í”¼ë²— í…Œì´ë¸” ìƒì„±
        pivot_table = df.pivot(index='Scenario', columns='Platform', values='Detection Rate')
        
        # íˆíŠ¸ë§µ ìƒì„±
        plt.figure(figsize=(10, 8))
        sns.heatmap(pivot_table, annot=True, cmap='YlOrRd', fmt='.3f', 
                   cbar_kws={'label': 'Detection Rate'})
        plt.title('ì‹œë‚˜ë¦¬ì˜¤ë³„ í”Œë«í¼ íƒì§€ìœ¨ íˆíŠ¸ë§µ')
        plt.xlabel('Platform')
        plt.ylabel('Scenario')
        plt.tight_layout()
        plt.savefig(self.results_dir / 'detection_pattern_heatmap.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def _create_roc_curves(self):
        """ROC ê³¡ì„  ìƒì„± (ê°€ëŠ¥í•œ ê²½ìš°)"""
        # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë” ë³µì¡í•œ ROC ê³¡ì„  ê³„ì‚°ì´ í•„ìš”
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ì˜ˆì‹œë§Œ ì œê³µ
        pass
    
    def generate_detailed_report(self) -> str:
        """ìƒì„¸ ë³´ê³ ì„œ ìƒì„±"""
        if not self.summary_data:
            return "ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        report = []
        report.append("# DID ìœ„í˜‘ íƒì§€ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ë³´ê³ ì„œ")
        report.append(f"**ìƒì„± ì‹œê°„**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("")
        
        # 1. í…ŒìŠ¤íŠ¸ ê°œìš”
        report.append("## 1. í…ŒìŠ¤íŠ¸ ê°œìš”")
        report.append(f"- **ì´ ì‹œë‚˜ë¦¬ì˜¤ ìˆ˜**: {self.summary_data.get('total_scenarios', 0)}")
        report.append(f"- **í…ŒìŠ¤íŠ¸ëœ í”Œë«í¼**: {', '.join(self.summary_data.get('platforms', []))}")
        report.append(f"- **í…ŒìŠ¤íŠ¸ ì‹œê°„**: {self.summary_data.get('test_timestamp', 'N/A')}")
        report.append("")
        
        # 2. ì „ì²´ ì„±ëŠ¥ ìš”ì•½
        overall_perf = self.summary_data.get('overall_performance', {})
        if overall_perf:
            report.append("## 2. ì „ì²´ ì„±ëŠ¥ ìš”ì•½")
            report.append(f"- **í‰ê·  Precision**: {overall_perf.get('avg_precision', 0):.3f}")
            report.append(f"- **í‰ê·  Recall**: {overall_perf.get('avg_recall', 0):.3f}")
            report.append(f"- **í‰ê·  F1-Score**: {overall_perf.get('avg_f1_score', 0):.3f}")
            report.append(f"- **í‰ê·  Accuracy**: {overall_perf.get('avg_accuracy', 0):.3f}")
            report.append("")
        
        # 3. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥
        report.append("## 3. ì‹œë‚˜ë¦¬ì˜¤ë³„ ì„±ëŠ¥")
        for scenario_name, scenario_summary in self.summary_data.get('scenario_summaries', {}).items():
            if 'error' in scenario_summary:
                report.append(f"### {scenario_name}")
                report.append(f"**ì˜¤ë¥˜**: {scenario_summary['error']}")
                report.append("")
                continue
            
            report.append(f"### {scenario_name}")
            report.append(f"- **í‰ê·  Precision**: {scenario_summary.get('avg_precision', 0):.3f}")
            report.append(f"- **í‰ê·  Recall**: {scenario_summary.get('avg_recall', 0):.3f}")
            report.append(f"- **í‰ê·  F1-Score**: {scenario_summary.get('avg_f1_score', 0):.3f}")
            report.append(f"- **í‰ê·  Accuracy**: {scenario_summary.get('avg_accuracy', 0):.3f}")
            report.append("")
            
            # í”Œë«í¼ë³„ ìƒì„¸ ê²°ê³¼
            report.append("#### í”Œë«í¼ë³„ ìƒì„¸ ê²°ê³¼")
            for platform_name, platform_result in scenario_summary.get('platform_results', {}).items():
                if 'error' in platform_result:
                    report.append(f"- **{platform_name}**: ì˜¤ë¥˜ - {platform_result['error']}")
                else:
                    report.append(f"- **{platform_name}**: F1={platform_result.get('f1_score', 0):.3f}, "
                                f"Acc={platform_result.get('accuracy', 0):.3f}, "
                                f"Detection Rate={platform_result.get('detection_rate', 0):.3f}")
            report.append("")
        
        # 4. ê¶Œì¥ì‚¬í•­
        report.append("## 4. ê¶Œì¥ì‚¬í•­")
        
        # ì„±ëŠ¥ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
        if overall_perf:
            avg_f1 = overall_perf.get('avg_f1_score', 0)
            if avg_f1 >= 0.8:
                report.append("- âœ… ì „ì²´ì ìœ¼ë¡œ ìš°ìˆ˜í•œ ì„±ëŠ¥ì„ ë³´ì…ë‹ˆë‹¤.")
            elif avg_f1 >= 0.6:
                report.append("- âš ï¸ ì„±ëŠ¥ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤. ëª¨ë¸ íŠœë‹ì„ ê³ ë ¤í•˜ì„¸ìš”.")
            else:
                report.append("- âŒ ì‹¬ê°í•œ ì„±ëŠ¥ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ëª¨ë¸ ì¬í›ˆë ¨ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        report.append("- ì •ê¸°ì ì¸ ëª¨ë¸ ì¬í›ˆë ¨ì„ í†µí•´ ì„±ëŠ¥ì„ ìœ ì§€í•˜ì„¸ìš”.")
        report.append("- ìƒˆë¡œìš´ ìœ„í˜‘ íŒ¨í„´ì— ëŒ€í•œ ì§€ì†ì ì¸ ëª¨ë‹ˆí„°ë§ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        report.append("- í”Œë«í¼ë³„ íŠ¹ì„±ì„ ê³ ë ¤í•œ ë§ì¶¤í˜• ìµœì í™”ë¥¼ ê³ ë ¤í•˜ì„¸ìš”.")
        report.append("")
        
        # 5. ê²°ë¡ 
        report.append("## 5. ê²°ë¡ ")
        report.append("MSL ê¸°ë°˜ DID ìœ„í˜‘ íƒì§€ ì‹œìŠ¤í…œì˜ í†µí•© í…ŒìŠ¤íŠ¸ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        report.append("ê° í”Œë«í¼ì—ì„œ ë‹¤ì–‘í•œ ìœ„í˜‘ ì‹œë‚˜ë¦¬ì˜¤ì— ëŒ€í•œ íƒì§€ ì„±ëŠ¥ì„ í‰ê°€í•˜ì˜€ìœ¼ë©°,")
        report.append("ì‹œìŠ¤í…œì˜ ì „ë°˜ì ì¸ ì„±ëŠ¥ê³¼ ê°œì„ ì ì„ íŒŒì•…í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.")
        
        return "\n".join(report)
    
    def save_analysis_results(self):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        # ì„±ëŠ¥ ë¶„ì„ ê²°ê³¼
        performance_analysis = self.analyze_performance_metrics()
        with open(self.results_dir / 'performance_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(performance_analysis, f, indent=2, ensure_ascii=False, default=str)
        
        # ìœ„í˜‘ íƒì§€ íŒ¨í„´ ë¶„ì„ ê²°ê³¼
        threat_analysis = self.analyze_threat_detection_patterns()
        with open(self.results_dir / 'threat_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(threat_analysis, f, indent=2, ensure_ascii=False, default=str)
        
        # ìƒì„¸ ë³´ê³ ì„œ ì €ì¥
        report = self.generate_detailed_report()
        with open(self.results_dir / 'detailed_report.md', 'w', encoding='utf-8') as f:
            f.write(report)
        
        logger.info("ë¶„ì„ ê²°ê³¼ ì €ì¥ ì™„ë£Œ")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì‹œì‘")
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™”
    analyzer = TestResultAnalyzer()
    
    # ê²°ê³¼ ë¡œë“œ
    if not analyzer.load_results():
        logger.error("ê²°ê³¼ ë¡œë“œ ì‹¤íŒ¨")
        return
    
    # ë¶„ì„ ì‹¤í–‰
    logger.info("ì„±ëŠ¥ ì§€í‘œ ë¶„ì„ ì¤‘...")
    performance_analysis = analyzer.analyze_performance_metrics()
    
    logger.info("ìœ„í˜‘ íƒì§€ íŒ¨í„´ ë¶„ì„ ì¤‘...")
    threat_analysis = analyzer.analyze_threat_detection_patterns()
    
    # ì‹œê°í™” ìƒì„±
    logger.info("ì‹œê°í™” ìƒì„± ì¤‘...")
    generated_files = analyzer.generate_visualizations()
    logger.info(f"ìƒì„±ëœ ì‹œê°í™” íŒŒì¼: {generated_files}")
    
    # ê²°ê³¼ ì €ì¥
    logger.info("ë¶„ì„ ê²°ê³¼ ì €ì¥ ì¤‘...")
    analyzer.save_analysis_results()
    
    logger.info("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ")
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "="*60)
    print("í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¶„ì„ ì™„ë£Œ")
    print("="*60)
    print(f"ğŸ“Š ë¶„ì„ ê²°ê³¼ íŒŒì¼:")
    print(f"   - performance_analysis.json")
    print(f"   - threat_analysis.json")
    print(f"   - detailed_report.md")
    print(f"ğŸ“ˆ ì‹œê°í™” íŒŒì¼:")
    for file in generated_files:
        print(f"   - {file}")
    print("="*60)


if __name__ == "__main__":
    main()