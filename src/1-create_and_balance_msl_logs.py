#!/usr/bin/env python3
"""
DID ìœ„í˜‘ íƒì§€ ì‹œìŠ¤í…œ - í†µí•© MSL ë¡œê·¸ ìƒì„± ë° ë°¸ëŸ°ì‹±
MSL ë¡œê·¸ ìƒì„±ë¶€í„° ë°ì´í„° ë°¸ëŸ°ì‹±ê¹Œì§€ í†µí•©ëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸
"""

from datetime import datetime, timedelta
import uuid
import random
import hashlib
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.utils.class_weight import compute_class_weight
from imblearn.over_sampling import SMOTE, RandomOverSampler
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTEENN, SMOTETomek
import numpy as np
import json
import asyncio
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional

# ì‹œë“œ ê³ ì •
random.seed(42)
np.random.seed(42)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# ===== MSL ìƒìˆ˜ ì •ì˜ =====
EVENT_TYPES = ["ISSUANCE", "PRESENTATION", "VERIFICATION", "REVOCATION"]

# DID ë° ì„œë¹„ìŠ¤ ì‹ë³„ì í’€
HOLDER_DIDS = [f"did:web:holder{i:04d}.identity.com" for i in range(1, 1001)]
VERIFIER_IDS = [
    "svc:bank-app-001", "svc:university-portal", "svc:hospital-system", 
    "svc:gov-service", "svc:insurance-portal", "svc:retail-app",
    "svc:travel-booking", "svc:enterprise-hr", "svc:fintech-app"
]
ISSUER_DIDS = [
    "did:web:issuer1.trusted.com", "did:web:university.edu", "did:web:gov.id",
    "did:web:issuer2.trusted.com", "did:web:hospital.org", "did:web:bank.com",
    "did:web:issuer3.untrusted.com", "did:web:fake-issuer.com"  # ì•…ì„± ë°œê¸‰ì
]

# ìœ„í˜‘ ì‹œë‚˜ë¦¬ì˜¤ ì •ì˜
THREAT_SCENARIOS = {
    "vc_reuse_attack": {
        "description": "ë™ì¼ VCë¥¼ ì—¬ëŸ¬ ê²€ì¦ìì—ê²Œ ì¬ì‚¬ìš©",
        "frequency": 0.05,
        "severity": "high"
    },
    "issuer_impersonation": {
        "description": "ê°€ì§œ ë°œê¸‰ìë¡œ ìœ„ì¡°ëœ VC ìƒì„±",
        "frequency": 0.03,
        "severity": "critical"
    },
    "revocation_ignore": {
        "description": "íê¸°ëœ VCë¥¼ ê³„ì† ì‚¬ìš©",
        "frequency": 0.02,
        "severity": "high"
    },
    "time_anomaly": {
        "description": "ë¹„ì •ìƒì ì¸ ì‹œê°„ íŒ¨í„´ (ë™ì‹œ ë‹¤ì¤‘ ì œì‹œ)",
        "frequency": 0.04,
        "severity": "medium"
    }
}

# ===== í†µí•© MSL ë°ì´í„° íŒŒì´í”„ë¼ì¸ =====
class IntegratedMSLPipeline:
    """í†µí•© MSL ë°ì´í„° ìƒì„± ë° ë°¸ëŸ°ì‹± íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.vc_registry = {}
        self.issued_vcs = set()
        self.revoked_vcs = set()
        self.original_distribution = {}
        self.balanced_distribution = {}
        
    def generate_vc_hash(self, holder_did, issuer_did, credential_type="degree"):
        """VC í•´ì‹œ ìƒì„± (ì¼ê´€ì„±ì„ ìœ„í•´ ê²°ì •ì )"""
        content = f"{holder_did}:{issuer_did}:{credential_type}:{uuid.uuid4().hex}"
        return f"blake3:{hashlib.blake2b(content.encode()).hexdigest()}"
    
    def generate_msl_event(self, event_type, threat_type="benign", base_time=None):
        """MSL í‘œì¤€ ì´ë²¤íŠ¸ ìƒì„±"""
        if base_time is None:
            base_time = datetime.now()
            
        # ì‹œê°„ ë³€ë™ (Â±30ì¼ ë²”ìœ„)
        timestamp = base_time + timedelta(
            days=random.randint(-30, 30),
            hours=random.randint(0, 23),
            minutes=random.randint(0, 59),
            seconds=random.randint(0, 59)
        )
        
        # ê¸°ë³¸ MSL í•„ë“œ
        holder_did = random.choice(HOLDER_DIDS)
        verifier_id = random.choice(VERIFIER_IDS)
        
        # ì´ë²¤íŠ¸ ìœ í˜•ë³„ ì²˜ë¦¬
        if event_type == "ISSUANCE":
            issuer_did = random.choice(ISSUER_DIDS)
            vc_hash = self.generate_vc_hash(holder_did, issuer_did)
            self.issued_vcs.add(vc_hash)
            self.vc_registry[vc_hash] = {
                "holder_did": holder_did,
                "issuer_did": issuer_did,
                "issued_at": timestamp,
                "status": "active"
            }
            
        elif event_type == "PRESENTATION":
            if threat_type == "vc_reuse_attack":
                vc_hash = random.choice(list(self.issued_vcs))
            elif threat_type == "revocation_ignore":
                vc_hash = random.choice(list(self.revoked_vcs)) if self.revoked_vcs else random.choice(list(self.issued_vcs))
            else:
                vc_hash = random.choice(list(self.issued_vcs)) if self.issued_vcs else self.generate_vc_hash(holder_did, random.choice(ISSUER_DIDS))
                
        elif event_type == "VERIFICATION":
            if threat_type == "issuer_impersonation":
                fake_issuer = random.choice([d for d in ISSUER_DIDS if "untrusted" in d or "fake" in d])
                vc_hash = self.generate_vc_hash(holder_did, fake_issuer)
            else:
                vc_hash = random.choice(list(self.issued_vcs)) if self.issued_vcs else self.generate_vc_hash(holder_did, random.choice(ISSUER_DIDS))
                
        elif event_type == "REVOCATION":
            if self.issued_vcs:
                vc_hash = random.choice(list(self.issued_vcs))
                self.issued_vcs.remove(vc_hash)
                self.revoked_vcs.add(vc_hash)
                if vc_hash in self.vc_registry:
                    self.vc_registry[vc_hash]["status"] = "revoked"
                    self.vc_registry[vc_hash]["revoked_at"] = timestamp
            else:
                vc_hash = self.generate_vc_hash(holder_did, random.choice(ISSUER_DIDS))
        
        # MSL Canonical Schema êµ¬ì¡°
        msl_event = {
            "event_id": str(uuid.uuid4()),
            "event_type": event_type,
            "vc_hash": vc_hash,
            "holder_did": holder_did,
            "verifier_id": verifier_id,
            "timestamp": timestamp.isoformat() + "Z",
            "optional": {
                "issuer_did": self.vc_registry.get(vc_hash, {}).get("issuer_did", random.choice(ISSUER_DIDS)),
                "anchor_status": self.vc_registry.get(vc_hash, {}).get("status", "unknown"),
                "device_id": f"tok:dev-{uuid.uuid4().hex[:8]}",
                "geo_token": f"tok:geo-{random.choice(['kr-seoul', 'us-ny', 'jp-tokyo', 'de-berlin'])}"
            },
            "label": threat_type,
            "threat_type": threat_type
        }
        
        return msl_event
    
    async def generate_threat_campaign(self, campaign_type, event_count, base_time):
        """ìœ„í˜‘ ìº í˜ì¸ ì´ë²¤íŠ¸ ìƒì„±"""
        events = []
        campaign_id = f"campaign-{uuid.uuid4().hex[:8]}"
        
        if campaign_type == "vc_reuse_campaign":
            issuance_event = self.generate_msl_event("ISSUANCE", "benign", base_time)
            vc_hash = issuance_event["vc_hash"]
            events.append(issuance_event)
            
            for i in range(event_count - 1):
                event_time = base_time + timedelta(minutes=i*5)
                presentation_event = self.generate_msl_event("PRESENTATION", "vc_reuse_attack", event_time)
                presentation_event["vc_hash"] = vc_hash
                presentation_event["campaign_id"] = campaign_id
                events.append(presentation_event)
                
        elif campaign_type == "issuer_impersonation_campaign":
            for i in range(event_count):
                event_time = base_time + timedelta(minutes=i*10)
                if i == 0:
                    event = self.generate_msl_event("ISSUANCE", "issuer_impersonation", event_time)
                else:
                    event = self.generate_msl_event("PRESENTATION", "issuer_impersonation", event_time)
                event["campaign_id"] = campaign_id
                events.append(event)
                
        elif campaign_type == "revocation_ignore_campaign":
            issuance_event = self.generate_msl_event("ISSUANCE", "benign", base_time)
            vc_hash = issuance_event["vc_hash"]
            events.append(issuance_event)
            
            revocation_event = self.generate_msl_event("REVOCATION", "benign", base_time + timedelta(hours=1))
            revocation_event["vc_hash"] = vc_hash
            events.append(revocation_event)
            
            for i in range(event_count - 2):
                event_time = base_time + timedelta(hours=2, minutes=i*15)
                presentation_event = self.generate_msl_event("PRESENTATION", "revocation_ignore", event_time)
                presentation_event["vc_hash"] = vc_hash
                presentation_event["campaign_id"] = campaign_id
                events.append(presentation_event)
        
        return events
    
    async def generate_msl_dataset(self) -> List[Dict]:
        """MSL ê¸°ë°˜ í¬ê´„ì  ë°ì´í„°ì…‹ ìƒì„±"""
        logger.info("MSL ê¸°ë°˜ DID ìœ„í˜‘ íƒì§€ ë°ì´í„°ì…‹ ìƒì„± ì‹œì‘...")
        
        base_time = datetime(2025, 1, 15, 12, 0, 0)
        all_events = []
        
        # 1. ì •ìƒ ì´ë²¤íŠ¸ ìƒì„±
        logger.info("ì •ìƒ ì´ë²¤íŠ¸ ìƒì„± ì¤‘...")
        for event_type in EVENT_TYPES:
            for _ in range(2000):
                event = self.generate_msl_event(event_type, "benign", base_time)
                all_events.append(event)
        
        # 2. ê°œë³„ ìœ„í˜‘ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„±
        logger.info("ìœ„í˜‘ ì‹œë‚˜ë¦¬ì˜¤ ìƒì„± ì¤‘...")
        for threat_type in THREAT_SCENARIOS.keys():
            logger.info(f"  - {threat_type}: 1000ê°œ")
            for event_type in EVENT_TYPES:
                for _ in range(250):
                    event = self.generate_msl_event(event_type, threat_type, base_time)
                    all_events.append(event)
        
        # 3. ìœ„í˜‘ ìº í˜ì¸ ìƒì„±
        logger.info("ìœ„í˜‘ ìº í˜ì¸ ìƒì„± ì¤‘...")
        campaigns = [
            ("vc_reuse_campaign", 500),
            ("issuer_impersonation_campaign", 300),
            ("revocation_ignore_campaign", 400)
        ]
        
        for campaign_name, count in campaigns:
            logger.info(f"  - {campaign_name}: {count}ê°œ")
            campaign_events = await self.generate_threat_campaign(campaign_name, count, base_time)
            all_events.extend(campaign_events)
        
        # 4. ë°ì´í„° ë°¸ëŸ°ìŠ¤ ì¡°ì •
        logger.info("âš–ï¸ ë°ì´í„° ë°¸ëŸ°ìŠ¤ ì¡°ì • ì¤‘...")
        current_threat_count = sum(1 for e in all_events if e["label"] != "benign")
        additional_benign = int(current_threat_count * 1.5)
        
        for _ in range(additional_benign):
            event_type = random.choice(EVENT_TYPES)
            event = self.generate_msl_event(event_type, "benign", base_time)
            all_events.append(event)
        
        return all_events
    
    def analyze_imbalance(self, df: pd.DataFrame) -> Dict[str, Any]:
        """ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„"""
        label_counts = df['label'].value_counts()
        total_samples = len(df)
        
        analysis = {
            'total_samples': total_samples,
            'class_distribution': label_counts.to_dict(),
            'class_percentages': (label_counts / total_samples * 100).to_dict(),
            'imbalance_ratio': label_counts.max() / label_counts.min(),
            'class_weights': compute_class_weight('balanced', classes=np.array(label_counts.index), y=df['label']).tolist()
        }
        
        return analysis
    
    def _encode_categorical_features(self, X: pd.DataFrame) -> pd.DataFrame:
        """ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©"""
        X_encoded = X.copy()
        categorical_columns = ['event_type', 'vc_hash', 'holder_did', 'verifier_id']
        
        for col in categorical_columns:
            if col in X_encoded.columns:
                X_encoded[col] = pd.Categorical(X_encoded[col]).codes
        
        return X_encoded
    
    def _reconstruct_dataframe(self, original_df: pd.DataFrame, X_resampled: np.ndarray, y_resampled: np.ndarray) -> pd.DataFrame:
        """ë¦¬ìƒ˜í”Œë§ëœ ë°ì´í„°ë¥¼ ì›ë³¸ DataFrame í˜•íƒœë¡œ ì¬êµ¬ì„±"""
        balanced_data = []
        
        for i in range(len(X_resampled)):
            original_idx = i % len(original_df)
            original_row = original_df.iloc[original_idx].copy()
            
            original_row['event_id'] = f"balanced_{i}_{original_row['event_id']}"
            original_row['label'] = y_resampled[i]
            original_row['threat_type'] = y_resampled[i]
            
            balanced_data.append(original_row)
        
        return pd.DataFrame(balanced_data)
    
    def balance_with_smote(self, df: pd.DataFrame, target_column='label') -> pd.DataFrame:
        """SMOTEë¥¼ ì‚¬ìš©í•œ ì˜¤ë²„ìƒ˜í”Œë§"""
        logger.info("SMOTEë¥¼ ì‚¬ìš©í•œ ë°ì´í„° ë°¸ëŸ°ì‹± ì‹œì‘...")
        
        X = df.drop(columns=[target_column, 'event_id', 'timestamp', 'optional', 'optional_json'])
        y = df[target_column]
        
        X_encoded = self._encode_categorical_features(X)
        
        smote = SMOTE(random_state=42, k_neighbors=3)
        X_resampled, y_resampled = smote.fit_resample(X_encoded, y)
        
        balanced_df = self._reconstruct_dataframe(df, X_resampled, y_resampled)
        
        logger.info(f"SMOTE ì™„ë£Œ: {len(df)} â†’ {len(balanced_df)} ìƒ˜í”Œ")
        return balanced_df
    
    def balance_with_random_oversampling(self, df: pd.DataFrame, target_column='label') -> pd.DataFrame:
        """ëœë¤ ì˜¤ë²„ìƒ˜í”Œë§"""
        logger.info("ëœë¤ ì˜¤ë²„ìƒ˜í”Œë§ ì‹œì‘...")
        
        X = df.drop(columns=[target_column, 'event_id', 'timestamp', 'optional', 'optional_json'])
        y = df[target_column]
        
        X_encoded = self._encode_categorical_features(X)
        
        ros = RandomOverSampler(random_state=42)
        X_resampled, y_resampled = ros.fit_resample(X_encoded, y)
        
        balanced_df = self._reconstruct_dataframe(df, X_resampled, y_resampled)
        
        logger.info(f"ëœë¤ ì˜¤ë²„ìƒ˜í”Œë§ ì™„ë£Œ: {len(df)} â†’ {len(balanced_df)} ìƒ˜í”Œ")
        return balanced_df
    
    def balance_with_undersampling(self, df: pd.DataFrame, target_column='label') -> pd.DataFrame:
        """ì–¸ë”ìƒ˜í”Œë§"""
        logger.info("ì–¸ë”ìƒ˜í”Œë§ ì‹œì‘...")
        
        X = df.drop(columns=[target_column, 'event_id', 'timestamp', 'optional', 'optional_json'])
        y = df[target_column]
        
        X_encoded = self._encode_categorical_features(X)
        
        rus = RandomUnderSampler(random_state=42)
        X_resampled, y_resampled = rus.fit_resample(X_encoded, y)
        
        balanced_df = self._reconstruct_dataframe(df, X_resampled, y_resampled)
        
        logger.info(f"ì–¸ë”ìƒ˜í”Œë§ ì™„ë£Œ: {len(df)} â†’ {len(balanced_df)} ìƒ˜í”Œ")
        return balanced_df
    
    def balance_with_hybrid(self, df: pd.DataFrame, target_column='label') -> pd.DataFrame:
        """SMOTE + ENN í•˜ì´ë¸Œë¦¬ë“œ ë°©ë²•"""
        logger.info("SMOTE + ENN í•˜ì´ë¸Œë¦¬ë“œ ë°¸ëŸ°ì‹± ì‹œì‘...")
        
        X = df.drop(columns=[target_column, 'event_id', 'timestamp', 'optional', 'optional_json'])
        y = df[target_column]
        
        X_encoded = self._encode_categorical_features(X)
        
        smote_enn = SMOTEENN(random_state=42)
        X_resampled, y_resampled = smote_enn.fit_resample(X_encoded, y)
        
        balanced_df = self._reconstruct_dataframe(df, X_resampled, y_resampled)
        
        logger.info(f"SMOTE + ENN ì™„ë£Œ: {len(df)} â†’ {len(balanced_df)} ìƒ˜í”Œ")
        return balanced_df
    
    def create_balanced_dataset(self, df: pd.DataFrame, method='smote') -> pd.DataFrame:
        """ì„ íƒëœ ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ì…‹ ë°¸ëŸ°ì‹±"""
        logger.info(f"{method} ë°©ë²•ìœ¼ë¡œ ë°ì´í„°ì…‹ ë°¸ëŸ°ì‹± ì‹œì‘...")
        
        if method == 'smote':
            return self.balance_with_smote(df)
        elif method == 'random_oversampling':
            return self.balance_with_random_oversampling(df)
        elif method == 'undersampling':
            return self.balance_with_undersampling(df)
        elif method == 'hybrid':
            return self.balance_with_hybrid(df)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°©ë²•: {method}")
    
    def split_and_save_data(self, df: pd.DataFrame, output_prefix: str = "msl_logs") -> Dict[str, str]:
        """ë°ì´í„° ë¶„í•  ë° ì €ì¥"""
        logger.info("ë°ì´í„° ë¶„í•  ë° ì €ì¥ ì¤‘...")
        
        # Train/Test ë¶„í• 
        train_df, test_df = train_test_split(
            df, test_size=0.2, stratify=df["label"], random_state=42
        )
        
        # íŒŒì¼ ê²½ë¡œ
        train_path = f"../data/train_{output_prefix}.csv"
        test_path = f"../data/test_{output_prefix}.csv"
        
        # ì €ì¥
        train_df.to_csv(train_path, index=False)
        test_df.to_csv(test_path, index=False)
        
        logger.info(f"Train ë°ì´í„°: {len(train_df):,}ê°œ â†’ {train_path}")
        logger.info(f"Test ë°ì´í„°: {len(test_df):,}ê°œ â†’ {test_path}")
        
        # Inference ë°ì´í„° ìƒì„±
        inference_dfs = []
        for label in df['label'].unique():
            label_data = df[df['label'] == label]
            sample_size = min(200, len(label_data))
            sample = label_data.sample(n=sample_size, random_state=42)
            inference_dfs.append(sample)
        
        inference_df = pd.concat(inference_dfs, ignore_index=True)
        inference_path = f"../data/inference_{output_prefix}.csv"
        inference_df.to_csv(inference_path, index=False)
        logger.info(f"Inference ë°ì´í„°: {len(inference_df)}ê°œ â†’ {inference_path}")
        
        return {
            'train': train_path,
            'test': test_path,
            'inference': inference_path
        }
    
    def save_metadata(self, df: pd.DataFrame, analysis: Dict[str, Any], 
                     balancing_method: str = None, file_prefix: str = "msl_dataset") -> str:
        """ë©”íƒ€ë°ì´í„° ì €ì¥"""
        metadata = {
            "ìƒì„±ì¼ì‹œ": datetime.now().isoformat(),
            "ë°ì´í„°ì…‹_ìœ í˜•": "MSL_DID_ìœ„í˜‘_íƒì§€",
            "MSL_í•„ë“œ": ['vc_hash', 'holder_did', 'verifier_id', 'timestamp'],
            "ì´ë²¤íŠ¸_ìœ í˜•": EVENT_TYPES,
            "ìœ„í˜‘_ì‹œë‚˜ë¦¬ì˜¤": list(THREAT_SCENARIOS.keys()),
            "ì „ì²´_ì´ë²¤íŠ¸ìˆ˜": len(df),
            "ë ˆì´ë¸”ë³„_ë¶„í¬": analysis['class_distribution'],
            "ì´ë²¤íŠ¸ìœ í˜•ë³„_ë¶„í¬": df['event_type'].value_counts().to_dict(),
            "ë¶ˆê· í˜•_ë¶„ì„": analysis,
            "ë°ì´í„°í’ˆì§ˆ": {
                "null_ê°’": int(df.isnull().sum().sum()),
                "ì¤‘ë³µ_ì´ë²¤íŠ¸": int(df.duplicated(subset=['event_id']).sum()),
                "ê³ ìœ _VCí•´ì‹œ": int(df['vc_hash'].nunique()),
                "ê³ ìœ _Holder_DID": int(df['holder_did'].nunique()),
                "ê³ ìœ _Verifier_ID": int(df['verifier_id'].nunique())
            }
        }
        
        if balancing_method:
            metadata["ë°¸ëŸ°ì‹±_ë°©ë²•"] = balancing_method
        
        metadata_path = f"../data/{file_prefix}_metadata.json"
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        logger.info(f"ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
        return metadata_path
    
    async def run_full_pipeline(self, enable_balancing: bool = True, 
                               balancing_method: str = 'random_oversampling') -> Dict[str, Any]:
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("=== í†µí•© MSL ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        
        # ë°ì´í„° ë””ë ‰í† ë¦¬ ìƒì„±
        Path("../data").mkdir(exist_ok=True)
        
        # 1. MSL ë°ì´í„°ì…‹ ìƒì„±
        events = await self.generate_msl_dataset()
        
        # 2. DataFrame ìƒì„± ë° ì •ë¦¬
        logger.info("ë°ì´í„° í›„ì²˜ë¦¬ ì¤‘...")
        df_full = pd.DataFrame(events)
        df_full = df_full.drop_duplicates(subset=['event_id'])
        df_full = df_full.dropna(subset=['timestamp', 'vc_hash', 'holder_did'])
        df_full['optional_json'] = df_full['optional'].apply(json.dumps)
        
        logger.info(f"ì „ì²´ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df_full):,}ê°œ")
        
        # 3. ì›ë³¸ ë°ì´í„° ë¶ˆê· í˜• ë¶„ì„
        original_analysis = self.analyze_imbalance(df_full)
        logger.info(f"ì›ë³¸ ë¶ˆê· í˜• ë¹„ìœ¨: {original_analysis['imbalance_ratio']:.1f}:1")
        
        # 4. ë°ì´í„° ë°¸ëŸ°ì‹± (ì„ íƒì )
        if enable_balancing:
            logger.info(f"ë°ì´í„° ë°¸ëŸ°ì‹± ì ìš©: {balancing_method}")
            
            # ë‹¤ì–‘í•œ ë°¸ëŸ°ì‹± ë°©ë²• ì‹œë„
            methods = ['smote', 'random_oversampling', 'undersampling', 'hybrid']
            balanced_datasets = {}
            
            for method in methods:
                try:
                    balanced_df = self.create_balanced_dataset(df_full, method)
                    balanced_analysis = self.analyze_imbalance(balanced_df)
                    balanced_datasets[method] = {
                        'data': balanced_df,
                        'analysis': balanced_analysis
                    }
                except Exception as e:
                    logger.error(f"{method} ë°¸ëŸ°ì‹± ì‹¤íŒ¨: {str(e)}")
            
            # ìµœì ì˜ ë°¸ëŸ°ì‹± ë°©ë²• ì„ íƒ
            if balanced_datasets:
                best_method = min(balanced_datasets.keys(), 
                                 key=lambda m: balanced_datasets[m]['analysis']['imbalance_ratio'])
                
                logger.info(f"ìµœì  ë°¸ëŸ°ì‹± ë°©ë²•: {best_method}")
                best_df = balanced_datasets[best_method]['data']
                best_analysis = balanced_datasets[best_method]['analysis']
                
                # ë°¸ëŸ°ì‹±ëœ ë°ì´í„° ë¶„í•  ë° ì €ì¥
                balanced_paths = self.split_and_save_data(best_df, f"msl_logs_balanced_{best_method}")
                
                # ë°¸ëŸ°ì‹± ë©”íƒ€ë°ì´í„° ì €ì¥
                balancing_metadata = {
                    "ë°¸ëŸ°ì‹±_ë°©ë²•": best_method,
                    "ì›ë³¸_ë¶ˆê· í˜•_ë¹„ìœ¨": original_analysis['imbalance_ratio'],
                    "ë°¸ëŸ°ì‹±_í›„_ë¶ˆê· í˜•_ë¹„ìœ¨": best_analysis['imbalance_ratio'],
                    "ê°œì„ _ì •ë„": original_analysis['imbalance_ratio'] / best_analysis['imbalance_ratio'],
                    "ì›ë³¸_ìƒ˜í”Œ_ìˆ˜": original_analysis['total_samples'],
                    "ë°¸ëŸ°ì‹±_í›„_ìƒ˜í”Œ_ìˆ˜": best_analysis['total_samples']
                }
                
                balancing_metadata_path = f"../data/balancing_metadata_{best_method}.json"
                with open(balancing_metadata_path, "w", encoding="utf-8") as f:
                    json.dump(balancing_metadata, f, ensure_ascii=False, indent=2)
                
                final_df = best_df
                final_analysis = best_analysis
                final_prefix = f"msl_logs_balanced_{best_method}"
            else:
                logger.warning("ë°¸ëŸ°ì‹± ì‹¤íŒ¨, ì›ë³¸ ë°ì´í„° ì‚¬ìš©")
                final_df = df_full
                final_analysis = original_analysis
                final_prefix = "msl_logs"
        else:
            logger.info("ë°ì´í„° ë°¸ëŸ°ì‹± ê±´ë„ˆëœ€")
            final_df = df_full
            final_analysis = original_analysis
            final_prefix = "msl_logs"
        
        # 5. ìµœì¢… ë°ì´í„° ë¶„í•  ë° ì €ì¥
        final_paths = self.split_and_save_data(final_df, final_prefix)
        
        # 6. ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata_path = self.save_metadata(final_df, final_analysis, 
                                         balancing_method if enable_balancing else None, 
                                         final_prefix)
        
        # 7. ê²°ê³¼ ìš”ì•½
        logger.info("=== íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
        logger.info(f"ìµœì¢… ë°ì´í„°: {len(final_df):,}ê°œ")
        logger.info(f"ë¶ˆê· í˜• ë¹„ìœ¨: {final_analysis['imbalance_ratio']:.1f}:1")
        
        return {
            'final_data': final_df,
            'final_analysis': final_analysis,
            'file_paths': final_paths,
            'metadata_path': metadata_path,
            'balancing_applied': enable_balancing,
            'balancing_method': balancing_method if enable_balancing else None
        }

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ í†µí•© MSL ë°ì´í„° ìƒì„± ë° ë°¸ëŸ°ì‹± íŒŒì´í”„ë¼ì¸")
    print("=" * 60)
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = IntegratedMSLPipeline()
    
    # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    results = asyncio.run(pipeline.run_full_pipeline(
        enable_balancing=True,
        balancing_method='random_oversampling'
    ))
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nâœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ğŸ“Š ìµœì¢… ë°ì´í„°: {len(results['final_data']):,}ê°œ")
    print(f"âš–ï¸ ë¶ˆê· í˜• ë¹„ìœ¨: {results['final_analysis']['imbalance_ratio']:.1f}:1")
    
    if results['balancing_applied']:
        print(f"ğŸ”„ ë°¸ëŸ°ì‹± ë°©ë²•: {results['balancing_method']}")
    
    print(f"ğŸ“ ìƒì„±ëœ íŒŒì¼:")
    for file_type, path in results['file_paths'].items():
        print(f"  - {file_type}: {path}")
    
    print(f"ğŸ“‹ ë©”íƒ€ë°ì´í„°: {results['metadata_path']}")

if __name__ == "__main__":
    main()